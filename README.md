# AI-Interview-Prep

---

### Table of Contents

<!-- TOC_START -->
| No. | Questions |
| --- | --------- |
| 1 | [What is flash attention in context to LLM](#what_is_flash_attention_in_context_to_llm) |
| 2 | [What is a prototype chain](#what_is_quantization?) |
| 3 | [What are the array mutation methods?](#what-are-the-array-mutation-methods) |
<!-- TOC_END -->

<!-- QUESTIONS_START -->
1. ### What is flash attention in context to LLM? 

   Flash Attention is a mechanism ...
2. ### What is quantization?

  Quantization is way to reduce the model size.....

3. ### What is the main difference between Horizontal and vertical quantization?
   Horizontal and vertical quantization ....
4. 
